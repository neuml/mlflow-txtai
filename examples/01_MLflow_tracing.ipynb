{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow automatic tracing for txtai\n",
    "\n",
    "This notebook demonstrates how to enable [MLflow](https://mlflow.org/) [automatic tracing](https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing) for `txtai`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "\n",
    "Install all dependencies. The txtai `graph` extra is also installed since one of the examples loads a graph index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%capture\n",
    "!pip install mlflow-txtai txtai[graph]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Initialization\n",
    "\n",
    " The first section initializes the environment. It assumes a mlflow server is running locally. That can be started as follows.\n",
    "\n",
    " ```\n",
    " mlflow server --host 127.0.0.1 --port 8000\n",
    " ```\n",
    "\n",
    " It also starts automatic tracing for `txtai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(uri=\"http://localhost:8000\")\n",
    "mlflow.set_experiment(\"txtai\")\n",
    "\n",
    "# Enable txtai automatic tracing\n",
    "mlflow.txtai.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textractor\n",
    "\n",
    "The first example traces a [Textractor pipeline](https://neuml.github.io/txtai/pipeline/data/textractor/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai.pipeline import Textractor\n",
    "\n",
    "with mlflow.start_run():\n",
    "    textractor = Textractor()\n",
    "    textractor(\"https://github.com/neuml/txtai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Next, we'll trace an [Embeddings](https://neuml.github.io/txtai/embeddings/) query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai import Embeddings\n",
    "\n",
    "with mlflow.start_run():\n",
    "    wiki = Embeddings()\n",
    "    wiki.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia-slim\")\n",
    "\n",
    "    embeddings = Embeddings(content=True, graph=True)\n",
    "    embeddings.index(wiki.search(\"SELECT id, text FROM txtai LIMIT 25\"))\n",
    "\n",
    "    embeddings.search(\"MATCH (A)-[]->(B) RETURN A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)\n",
    "\n",
    "The next example traces a [RAG pipeline](https://neuml.github.io/txtai/pipeline/text/rag/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai import Embeddings, RAG\n",
    "\n",
    "with mlflow.start_run():\n",
    "    wiki = Embeddings()\n",
    "    wiki.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia-slim\")\n",
    "\n",
    "    # Define prompt template\n",
    "    template = \"\"\"\n",
    "    Answer the following question using only the context below. Only include information\n",
    "    specifically discussed.\n",
    "\n",
    "    question: {question}\n",
    "    context: {context} \"\"\"\n",
    "\n",
    "    # Create RAG pipeline\n",
    "    rag = RAG(\n",
    "        wiki,\n",
    "        \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\",\n",
    "        system=\"You are a friendly assistant. You answer questions from users.\",\n",
    "        template=template,\n",
    "        context=10\n",
    "    )\n",
    "\n",
    "    rag(\"Tell me about the Roman Empire\", maxlength=2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "\n",
    "This example runs a [workflow](https://neuml.github.io/txtai/workflow/). This workflow runs an embeddings query and then translates each result to French. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai import Embeddings, Workflow\n",
    "from txtai.pipeline import Translation\n",
    "from txtai.workflow import Task\n",
    "\n",
    "with mlflow.start_run():\n",
    "    wiki = Embeddings()\n",
    "    wiki.load(provider=\"huggingface-hub\", container=\"neuml/txtai-wikipedia-slim\")\n",
    "\n",
    "    # Translation instance\n",
    "    translate = Translation()\n",
    "\n",
    "    workflow = Workflow([\n",
    "        Task(lambda x: [y[0][\"text\"] for y in wiki.batchsearch(x, 1)]),\n",
    "        Task(lambda x: translate(x, \"fr\"))\n",
    "    ])\n",
    "\n",
    "    print(list(workflow([\"Roman Empire\", \"Greek Empire\", \"Industrial Revolution\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "The last example runs a [txtai agent](https://neuml.github.io/txtai/agent/) designed to research questions on astronomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txtai import Agent, Embeddings\n",
    "\n",
    "def search(query):\n",
    "    \"\"\"\n",
    "    Searches a database of astronomy data.\n",
    "\n",
    "    Make sure to call this tool only with a string input, never use JSON.    \n",
    "\n",
    "    Args:\n",
    "        query: concepts to search for using similarity search\n",
    "\n",
    "    Returns:\n",
    "        list of search results with for each match\n",
    "    \"\"\"\n",
    "\n",
    "    return embeddings.search(\n",
    "        \"SELECT id, text, distance FROM txtai WHERE similar(:query)\",\n",
    "        10, parameters={\"query\": query}\n",
    "    )\n",
    "\n",
    "embeddings = Embeddings()\n",
    "embeddings.load(provider=\"huggingface-hub\", container=\"neuml/txtai-astronomy\")\n",
    "\n",
    "agent = Agent(\n",
    "    tools=[search],\n",
    "    llm=\"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\",\n",
    "    max_iterations=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher = \"\"\"\n",
    "{command}\n",
    "\n",
    "Do the following.\n",
    " - Search for results related to the topic.\n",
    " - Analyze the results\n",
    " - Continue querying until conclusive answers are found\n",
    " - Write a Markdown report\n",
    "\"\"\"\n",
    "\n",
    "with mlflow.start_run():\n",
    "    agent(researcher.format(command=\"\"\"\n",
    "    Write a detailed list with explanations of 10 candidate stars that could potentially be habitable to life.\n",
    "    \"\"\"), maxlength=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
